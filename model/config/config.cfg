[paths]
train = "data/train.spacy"
dev = "data/dev.spacy"

[system]
gpu_allocator = null
seed = 0

[nlp]
lang = "en"
pipeline = ["tok2vec","ner"]
# Default batch size to use with nlp.pipe and nlp.evaluate
batch_size = 1000
# components which are loaded but disabled by deafult
disabled = [] 
# Optional callbacks to modify the nlp object before it's initialized, after
# it's created and after the pipeline has been set up
before_creation = null
after_creation = null
after_pipeline_creation = null
tokenizer = {"@tokenizers":"spacy.Tokenizer.v1"}

[components]

[components.ner]
# Name of trained pipeline to copy components from
source = "en_core_med7_lg"
# Don't think we need this as we have only one 
# component after tok2vec but it shouldn't hurt
replace_listeners = ["model.tok2vec"]
# Allow new ents AS_REQUIRED and AS_DIRECTED to propagate
overwrite_ents = True

[components.tok2vec]
# Take from med7
source = "en_core_med7_lg"

[corpora]

[corpora.dev]
# Function that creates a Corpus
@readers = "spacy.Corpus.v1"
path = ${paths.dev}
# Whether to set up Example object with gold-standard sentences and tokens for the predictions
gold_preproc = false
# 0 means no limit
max_length = 0
# If non-zero, limits corpus to a subset of examples
limit = 0
# Can replace tokens with variations by adding data augmentation
augmenter = null

[corpora.train]
@readers = "spacy.Corpus.v1"
path = ${paths.train}
gold_preproc = false
max_length = 0
limit = 0
augmenter = null

[training]
dev_corpus = "corpora.dev"
train_corpus = "corpora.train"
seed = ${system.seed}
gpu_allocator = ${system.gpu_allocator}
# proportion of nodes to be randomly dropped
dropout = 0.1
accumulate_gradient = 1
patience = 3600
max_epochs = 0
max_steps = 20000
# How often to evalute during training
eval_frequency = 200
# Don't update this during training
frozen_components = ["tok2vec"]
# Components that set annotations for following components
annotating_components = []
# Can modify nlp object right before saving to disk
before_to_disk = null

# Creates mini-batches of Docs for use in gradient descent
[training.batcher]
# default batcher spacy.batch_by_words.v1
@batchers = "spacy.batch_by_words.v1"
discard_oversize = false
# What percentage of the size to allow batches to exceed
tolerance = 0.2
get_length = null

# Target number of words per batch
[training.batcher.size]
# Yields infinite series of compounding values
# each time the generator is called a value is produced by
# multiplying the previous value by the compound rate
# i.e. here 100, 100.1, 100.2, 100.3, 100.3*1.001 etc.
@schedules = "compounding.v1"
start = 100
stop = 1000
compound = 1.001
t = 0.0

[training.logger]
@loggers = "spacy.ConsoleLogger.v1"
progress_bar = false

[training.optimizer]
@optimizers = "Adam.v1"
beta1 = 0.9
beta2 = 0.999
# AdamW style algorithm if true
L2_is_weight_decay = true
L2 = 0.01
# Clip to avoid exploding gradients
grad_clip = 1.0
use_averages = false
eps = 0.00000001
learn_rate = 0.001

[training.score_weights]
ents_f = 1.0
ents_p = 0.0
ents_r = 0.0
# Not sure why we need to include the line below - could try removing
ents_per_type = null

# Don't need to pretrain as starting from med7 model
[pretraining]

[initialize]
vectors = null
vocab_data = null
after_init = null
lookups = null

[initialize.before_init]
# Copy tokeniser and vocab from specified model
@callbacks = "spacy.copy_from_base_model.v1"
tokenizer = "en_core_med7_lg"
vocab = "en_core_med7_lg"

[initialize.components]

[initialize.tokenizer]